% Chapter 6

\chapter{Evaluation}% Main chapter title

\label{Chapter6} % For referencing the chapter elsewhere, use \ref{Chapter6} 
In this chapter, experiments are performed to determine whether the propositions of the database model and the database application design in Chapter 4 and Chapter 5 meet the user requirements. The experiments are real-time data collection and an overnight data collection, importing data from EDF file format, exporting data from the database to EDF file format, and querying data from the database. When performing a database evaluation, the most important metrics under evaluating are the read/write speed and the size of the database. That is, time to retrieve data and space to store data must meet the user requirements. Moreover, the database application must ensure no data lost, or data corrupted when storing and retrieving. With respect to mobile platforms, which are usually limited resources and battery driven, the database application must also use the resources in an efficient way. Since a database connection is shared for many activities while the application is used, it needs to evaluate time each activity uses the connection.\\
The extensibility for future usage is clearly presented in the design chapter, and therefore not be evaluated in this section. As discussed in the design chapter, the database design is a platform independent, it can easily be implemented on a bio-signals collector application such as CESAR acquisition tool to minimize the sending and receiving overhead between collectors and the database application server. There are some common experiments when evaluating a database design and database application. The evaluated experiments in this chapter, therefore, inspire applications which want to implement the database design.\\
The goals of the experiments are to evaluate the feasibility of the database model, and to convince the reader that the functional and non-functional requirements for the database modeling and the database application are satisfied by measuring performance of the read/write on the database, performance of the database application, and storage capacity. In this section, each experiment is evaluated with respect to reasons it is performed, workloads used, evaluated metrics, procedure to evaluate, and the results from the experiment.\\
A BITalino plugged kit, two Android devices, and a CESAR acquisition simulator run on macOS Sierra are using for the experiments. Table \ref{tab:DevicesSpecs} presents specifications for the used devices in this chapter.
\section{Real-time data collecting experiment}
\begin{table}
\begin{center}
\begin{tabular}{ |p{2cm}|p{2.7cm}|p{2.8cm}|p{3cm}|}
 \hline
 Technology&Xiaomi Mi5& MacBook Retina\newline 15 Late 2013&BITalino BT\newline plugged kit\\
 \hline
 OS&Android 7.0& macOS Sierra&n/a\\
 \hline
 RAM&3GB&16GB&n/a\\
 \hline
 CHIP&Snapdragon 820\newline 4 core 1.8GHz&Intel core i7\newline 4 core 2.3GHz&MCU with sampling rate 1, 10, 100, or 1000Hz\\
 \hline
 Wi-Fi&802.11\newline a/b/g/n/ac& 802.11\newline a/b/g/ac&n/a\\
 \hline
 Bluetooth&v4.2&v4.0&v2.0, range 10m\\
 \hline
 Battery&3000mAh&8440mAh&500mAh or 1500mAh\\
 \hline
\end{tabular}
\end{center}
\caption{Used devices specifications}
\label{tab:DevicesSpecs}
\end{table}
The database application is required to collect and store bio-signal data from the CESAR acquisition tool to the designed database model on a mobile device. The CESAR acquisition tool sends data with four different frequencies, that are 1Hz, 10Hz, 100Hz, and 1000Hz. The goal of the database application is to collect OSA data, hence, a duration of the experiment can be more than 12 hours. To prove that the application is satisfied for a long time collection, an overnight experiment is presented in the next section. Currently, the CESAR acquisition tool cannot delivery samples at 1000Hz, it stops sending data after 30min-40min. Therefore, a CESAR acquisition simulator is used for sending samples with frequencies 1000Hz or higher. Goals of this experiment are to evaluate the database read/write performances and database size when collecting data from many different sources with different frequencies. Since the application is run on a mobile platform where the resources are limited, usage resources such as CPU and battery consumption are also evaluated in this section.
\subsection{Experiment workloads}
The database application collects samples from multiple sources simultaneously, in which each source sends data at different frequencies. A mount of samples the database need to store at a certain time is depended on the number of current connected sources and the number of channels that belong to each source. In addition, when the frequencies are high, the number of arriving samples in a period time becomes very large; it has a huge impact on the storing process of the database application. Since the application uses batch processing to manage and store incoming samples, a batch size highly influences the performance of the application. The batch size is defined by multiplying the buffer duration with arrival rate. Therefore, the workloads for the experiment are a buffer duration in second, an arrival rate of a source in Hz, a number of channels (total channels from the connected sources).
\subsection{Experiment metrics}
Evaluation metrics must highlight the goals and the workloads of the experiment. In the real-time data collecting experiment, the application has to parse arrival packages into relevant objects for the database. Therefore, percent CPU usage and power consumption are considered the metrics used in the evaluation. Percent CPU usage is obtained by using command “adb shell busybox top –d 5”. This command generates an output which contains the information about CPU usage for each application, and the output is printed in the standard output in each five seconds. The outputs from the command are redirected to a text file, and the file are presented in charts. The python code for parsing CPU usage texts file into a chart is presented in Appendix \ref{AppendixC}, in which Listing \ref{listing:BUSYBOXTOPTEXT} presents a sample output from the command, and a discussion about how to parse the output is also illustrated in the appendix. Power consumption can be obtained after a collecting process is finished by checking the battery usage in the Android system. Figure \ref{fig:Figures/PowerMetric} presents an overnight experiment sample where the power consumption for each application is calculated in percent of total power drained off.\\
Since the application uses batch processing to manage arrival data, when the temporary buffer is full, a SQLite transaction is requested for inserting data into the database. While the transaction is performed, it locks the entire database file. Hence, the SQLite time usage in millisecond for different batch sizes, and the number of insertions per millisecond are another metrics for the experiment. Nonetheless, the application is implemented on the mobile platform where resources are limited, it is good to see how big the database grows in megabytes for different workloads in a specific period time.
\subsection{Experiment setup}
As described above, there are two workload generators that are used in the experiment, they are the CESAR acquisition tool, and the CESAR acquisition simulator. In addition, a workload which is the buffer duration for the application can be adjusted before performing a collecting process. A workload that is monitored (its value is adjusted before a collecting process starts) is called the experimental workload. Other workloads that are unmonitored (their values must be keep the same for each collecting process) are called control workloads. Calculated results after each collecting process are called the responsive metrics.\\
Table \ref{tab:BufferDuration} presents the responsive metrics for an experiment where the experimental workload is a buffer duration; control workloads are arrival rate and number of used channels. In this experiment, arrival rate is 100Hz, and all channels of the CESAR acquisition tool are used (6 channels). Likewise, in Table \ref{tab:ArrivalRate}, the experimental workload is arrival rate; control workloads are buffer duration (10 second), and all channels of the CESAR acquisition tool are used (6 channels). The number of channels is chosen as the experimental workload in Table \ref{tab:NrOfChannels}. In this case, the control workloads are the buffer duration (10s) and arrival rate (100Hz).\\
For each experiment, a collecting process is iterated three times with a specific experimental workload value. That is, experimental workloads for the buffer duration (batch size) are 10 seconds, 20 seconds, and 30 seconds; experimental workloads for the arrival rate are 1Hz, 10Hz, 100Hz, and 1000Hz; experimental workloads for number of channels are 1 channels, 3 channels and 6 channels. The metrics that are evaluated for these iterations are the power consumption, SQLite usage time, and the size of the database after the iterations.\\
The duration for each iteration is about 60 minutes. Because the main metrics for the experiment are the SQLite write performance, resource usage, and the size of the collected data in the database, it does not need to perform a long time run for each iteration. In stead, only one overnight experiment is provided to prove that the database application is satisfied to use for storing data from a long time run. The Xiaomi Mi5 phone is connected to the MacBook such that the percent CPU usage of the phone are sent to the MacBook. That is, an adb shell busybox command is run on the Terminal of the MacBook, and the percent CPU usage from the adb are redirected to a text file which is stored in the root of the folder project of the database application. The percent of power consumption is obtained from the "Battery use" of the Android operating system. However, the iteration needs to be repeated without connecting to the MacBook or a power adapter since the "Battery use" does not perform the calculation unless the phone is powered by the battery. The database size in megabytes is obtained by getting the length of the database file, then dividing for 1024*1024 (converting from byte to megabyte). Listing \ref{listing:GETDBSIZE} presents how to get the SQLite database size in the Android. To calculate SQLite time usage, a timer is set before each SQL insert transaction, by adding all results from the timers, the SQLite time usage is obtained.
\begin{table}
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Responsive metrics}} & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}c@{}}Experimental workload\\ Buffer duration\end{tabular}} \\ \cline{2-4} 
\multicolumn{1}{|c|}{} & 10 seconds & 20 seconds & 30 seconds \\ \hline
\begin{tabular}[c]{@{}l@{}}CPU usage\\ (percent with 5s segment)\end{tabular} & RTbd1.txt & RTbd2.txt & RTbd3.txt \\ \hline
\begin{tabular}[c]{@{}l@{}}Power consumption\\ (percent)\end{tabular} & 3.0 &  &  \\ \hline
\begin{tabular}[c]{@{}l@{}}SQLite time usage\\ (millisecond)\end{tabular} & 194 242 & 193 130 & 184 811 \\ \hline
\begin{tabular}[c]{@{}l@{}}Duration\\ (millisecond)\end{tabular} & 3 668 780 & 3 750 028 & 3 711 576 \\ \hline
\begin{tabular}[c]{@{}l@{}}Database size\\ (Megabytes)\end{tabular} & 90 & 91 & 91 \\ \hline
\end{tabular}
\caption{Buffer duration}
\label{tab:BufferDuration}
\end{table}
\begin{table}
\centering
\begin{tabular}{|l|c|c|c|l|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Responsive metrics}} & \multicolumn{4}{c|}{\begin{tabular}[c]{@{}c@{}}Experimental workload\\ Arrival rate\end{tabular}} \\ \cline{2-5} 
\multicolumn{1}{|c|}{} & 1 Hz & 10 Hz & 100 Hz &700Hz (simulator)\\ \hline
\begin{tabular}[c]{@{}l@{}}CPU usage\\ (percent with 5s segment)\end{tabular} & RTar1.txt & RTar2.txt & RTar3.txt & RTar4.txt \\ \hline
\begin{tabular}[c]{@{}l@{}}Power consumption\\ (percent)\end{tabular} &  &  & 3.0 &  \\ \hline
\begin{tabular}[c]{@{}l@{}}SQLite time usage\\ (millisecond)\end{tabular} & 14 311 & 39 442 & 194 242 & 2 012 042 \\ \hline
\begin{tabular}[c]{@{}l@{}}Duration\\ (millisecond)\end{tabular} & 3 658 002 & 3 679 001 & 3 668 780 & 3 737 474 \\ \hline
\begin{tabular}[c]{@{}l@{}}Database size\\ (Megabytes)\end{tabular} & < 1 & 9 & 90 & 653 \\ \hline
\end{tabular}
\caption{Arrival rate}
\label{tab:ArrivalRate}
\end{table}
\begin{table}
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Responsive metrics}} & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}c@{}}Experimental workload\\ Number of channels\end{tabular}} \\ \cline{2-4} 
\multicolumn{1}{|c|}{} & 1 & 3 & 6 \\ \hline
\begin{tabular}[c]{@{}l@{}}CPU usage\\ (percent with 5s segment)\end{tabular} & RTnrc1.txt & RTnrc3.txt & RTnrc6.txt \\ \hline
\begin{tabular}[c]{@{}l@{}}Power consumption\\ (percent)\end{tabular} &  &  &  \\ \hline
\begin{tabular}[c]{@{}l@{}}SQLite time usage\\ (millisecond)\end{tabular} & 54 613 &  & 194 242 \\ \hline
\begin{tabular}[c]{@{}l@{}}Duration\\ (millisecond)\end{tabular} &  &  & 3 668 780 \\ \hline
\begin{tabular}[c]{@{}l@{}}Database size\\ (Megabytes)\end{tabular} & 15 & 45 & 90 \\ \hline
\end{tabular}
\caption{Number of channels}
\label{tab:NrOfChannels}
\end{table}
\begin{minipage}{\linewidth}{}
\begin{lstlisting}[caption={Get the size of a SQLite database in the Android}, label = {listing:GETDBSIZE}, captionpos=b, basicstyle=\ttfamily\footnotesize]
File f = getApplicationContext().getDatabasePath(OSADBHelper.DATABASE_NAME);
long dbSize = f.length()/(1024*1024);
Toast.makeText(getApplicationContext(),"Database size "+String.valueOf(dbSize)
               +" MB",Toast.LENGTH_SHORT).show();
\end{lstlisting}
\end{minipage}
\subsection{Results and discussion}


\section{Overnight experiment}
Overnight experiment is performed in this thesis for proving that the database application can collect data for a long period period of time. In addition, the feasibility of the database model and database application for monitoring sleep during a whole night is also proved by assessing metrics such as the power usage, the size of the collected data, time that the database application accessing the database file, etc. In this experiment, the database application listens to data from the CESAR acquisition tool until the tool or a users disconnects the connection.
\subsection{Experiment workloads and metrics}
Workloads, that are used in this experiment, are all channels from the BITalino sensor kit (6 channels). Each channel collects data with 100Hz frequency, which is 100 samples per second. Therefore, total samples the database application need to put to the database in one second is 600 samples. To avoid overhead when accessing the database file for storing samples, the database application uses batch processing. The batch size that is used in this experiment is 5 seconds. That is, the application groups 5*600 = 3000 samples into one insert transaction (instead of 3000 transactions if the batch processing is not used). Used metrics for this experiment are the percent power consumption of the total power usage of the mobile device, size of the collected data in megabytes, number of samples in the database, number of received packets, percent time accessing the database file over the experiment duration.
\subsection{Experiment setup}
The BITalino can be powered by different battery capacities; a 500mAh and 1500mAh, which can offer about 6 to 20 hours for collecting data with 100Hz each channel. However, only 1500mAh is used in this experiment, because 500mAh battery can not be used for performing the overnight experiment. The 1500mAh battery of the BITalino sensor kit is full charged. In addition, the Xiaomi Mi5 phone is plugged to a power adapter, because the current version of the CESAR acquisition tool need the display of the phone is on to avoid killed by the Android system for a long time running. This configuration does not impact the experiment, since the metric that is used for evaluating the power usage, is the percent power the application consumed over the total used power during the experiment. The phone is full charged before the experiment, then the percent power consumption of the database application is gotten when the battery power is about 50\%. In other words, the percent power usages for applications, which are running under the experiment, are stable, therefore, the power usage of the database application can be evaluated. Total SQLite time usage is calculated by adding all timed insertions of each transaction from the batch processing. Duration is calculated by subtracting the maximum timestamp and the minimum timestamp of the samples in the overnight record. Percent of power consumption is from the "battery use" which can be found in the settings of the Android phone. Number of packets can be gotten from either the CESAR acquisition tool, or counting number of samples in the database dividing to number of channels that are used in the experiment (6 channels). To get the number of collected samples, a simple SQL query is performed, i.e., "SELECT COUNT(*) FROM SAMPLE". Since the database is empty before the experiment is performed, the database size is also the size of all data collected after the experiment performed. The results from the experiment are presented in Table \ref{tab:OvernightExperiment}.
\begin{table}
\centering
\begin{tabular}{|l|r|}
\hline
Start time & 20.04.2017 17:43:55 \\ \hline
End time & 21.01.2017 12:32:20 \\ \hline
Duration & 67 705 166ms = about 18 hours, 48min \\ \hline
\begin{tabular}[c]{@{}l@{}}SQLite time usage\\ (millisecond)\end{tabular} & 4 251 412 ms \\ \hline
\begin{tabular}[c]{@{}l@{}}Percent SQLite time usage\\ over the duration\end{tabular} & 4 251 412/ 67 705 166 = about 6.3\% \\ \hline
\begin{tabular}[c]{@{}l@{}}Power consumption\\ (percent)\end{tabular} & 6.6\% of total power usage \\ \hline
\begin{tabular}[c]{@{}l@{}}Number packet\\ from the CESAR\\ acquisition tool\end{tabular} & 6 770 896 packets \\ \hline
Number of samples & 40 625 376 \\ \hline
\begin{tabular}[c]{@{}l@{}}Database size\\ (Megabytes)\end{tabular} & 1736 MB \\ \hline
\end{tabular}
\caption{Overnight experiment}
\label{tab:OvernightExperiment}
\end{table}
\subsection{Results and discussion}
For the write performance of the database application, as presented in Table \ref{tab:OvernightExperiment}, number of samples are collected under the experiment is 40625376, and the total time used for insert transactions is 4251412 millisecond. Based on these numbers, an average used time for each transaction is 4251412/(40625376/3000) = 314 millisecond, where 40625376/3000 = 13542 is total insert transactions performed. That is, the database uses 314 millisecond to store samples from 5000 millisecond (5 second buffer). Hence, the percent time usage for accessing the database file (I/O time usage) is (314/5000)*100\% = about 6.3\% of the idle, it can also be calculated by using the total SQLite time usage and the duration as presented in Table \ref{tab:OvernightExperiment}. Based on the percent time usage, the database application can manage up to 15 BITalino sensor kits where all channels of the kits are used, and the I/O time usage to access the database file is 15*6.3\% = 94,5\% idle. 15 BITalino sensor kits offer 15*6 = 90 different sensors that can be used to collect data at 100Hz, or 15 users can use one Xiaomi MI5 phone to store data simultaneously.\\
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/PowerMetric.png}
    \caption{Battery usage under overnight experiment}
    \label{fig:Figures/PowerMetric}
\end{figure}
For the power consumption, the database application does not use much battery compared to other applications. Figure \ref{fig:Figures/PowerMetric} presents the power consumption for applications that are running under the experiment. The CESAR acquisition tool consumes about 31.1\% while the database application consumes 6.6\% of total power usage. Hence, the application is considered quite efficient in the case of power usage.\\
The overnight experiment is performed during 18 hours and 48 minutes, which is longer than a normal nighttime sleep. The data size after collecting data for nearly 19 hours is 1736 mebabytes (1.7 GB). Currently, an Android phone is usually has 16 to 64 gigabytes(GB) internal memory, in addition, an external Secure Digital (SD) card with 16GB costs about 80kr. With a 16GB SD card, a user can store up to nine 18 hours (overnight) records, or nine users can use the same SD card to store their collected data. The phone that is used in the experiment has 32GB memory which offers 15 users use it simultaneously. It is 15 users because the write performance from the previous discussion can handle up to 15 users.\\
Nonetheless, the results from the experiment present that the database application is satisfied to be used in a long period time of collection. Moreover, multiple users can share an Android mobile device to store their data. Furthermore, the power consumption and data size acceptable are quietly efficient when collecting data.
\section{EDF importing experiment}
The database model is designed in the way that opens for all bio-signals, and can store data from other database sources if the sources support EDF exporting. In other words, the data base application is required to import data from EDF and EDF+ file formats. Goals of the experiment are to evaluate database writing performance and usage resources such as CPU and battery consumption, especially, when the application reads files simultaneously.
\subsection{Experiment workloads}
The database application reads sample data from multiple files simultaneously. Ideally, each sample from the files is parsed to an object in memory, then inserted into the database. However, each INSERT statement in SQLite has its own transaction, and SQLite can only do a few dozen transactions per second. The samples are, therefore, grouped into a transaction before performing a SQLite insertion. The transaction locks entire database file when it performs inserting. Hence, if the sample group is too large, it can cause either memory problem, or poor performance since other transactions have to wait for it. Therefore, the workloads for the experiment are a size of the sample group and the number of file read.
\subsection{Experiment metrics}
Metrics that are chosen to highlight the goals and workload of the experiment are the percent CPU usage, the percent power consumption, the size of a EDF file in the database, and total time the experiment holds the shared SQLite connection. The percent CPU usage and the power consumption need to be evaluated since the application need to parse the samples in the EDF file format to objects that can be stored into the database. In addition, the time that used to insert samples can be used for evaluating the writing performance of the database.
\subsection{Experiment setup}
\begin{lstlisting}[caption={Convert mit2edf by using terminal}, label = {listing:MIT2EDF}, captionpos=b]
(no options)Terminal $mit2edf -r a01 
(options)Terminal $mit2edf -r a01 [-v -h -p] -o phy1.edf
\end{lstlisting}
To generate workloads for the experiment, a function from Physionet which named mit2edf is used for exporting bio-signal data from Physionet databases to EDF file format. As presented in the manual page of the function, mit2edf creates an EDF file that contains the same data as the input files which are in form of Waveform Database format (header and signal files). However, the function is included in WFDB software package from Physionet, the user must install the package first. There are three options when using the function which are printing a brief usage summary, choosing a name for the exported edf (the exported file has the same name as the imported record), and printing debugging output. Listing \ref{listing:MIT2EDF} presents how to use the function to convert a01 record to EDF file format without options and with options on a Linux/Unix terminal.
\begin{table}
\centering
\begin{tabular}{|l|l|r|r|r|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Responsive metrics\end{tabular}}} & \multicolumn{3}{l|}{\begin{tabular}[c]{@{}c@{}}Experimental workload\\ Number of files simultaneously read\end{tabular}} \\ \cline{3-5} 
\multicolumn{2}{|l|}{} & 1 (5,64 MB) & 2 (11,71 MB) & 3 (17,69 MB) \\ \hline
\multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}}CPU usage\\ (percent)\end{tabular}} & edfFI1.txt & edfFI2.txt & edfFI3.txt \\ \hline
\multicolumn{2}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Power consumption\\ (percent)\end{tabular}}} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} \\
\multicolumn{2}{|l|}{} & 2.4 & 4.8 & 6.6 \\ \hline
\multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}}SQLite time usage\\ (millisecond)\end{tabular}} & 228 235 & 466 203 & 703 837 \\ \hline
\multicolumn{2}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Database file size\\ (Megabytes)\end{tabular}}} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} \\
\multicolumn{2}{|l|}{} & 100 & 218 & 335 \\ \hline
\end{tabular}
\caption{Number of files simultaneously read}
\label{tab:NrOfSIMULTANEOUSELYREAD}
\end{table}
Workloads that are used for the experiment are a01.edf (5,64 MB), a02.edf (6,07 MB), and a03.edf (5,98 MB) files, which are generated from a01, a02, and a03 records from CinC2000 data sets. Since the implementation for the EDF-reader uses the same algorithm (batching processing) as real-time importing, there are two factors that influent the goals of the experiments. They are the number of simultaneously EDF files read, and the buffer size (batch size). Table \ref{tab:NrOfSIMULTANEOUSELYREAD} presents the results for the experiment where the experimental workload is the number of simultaneously EDF files read. In this experiment, the batch size is 50000 samples, and is kept the same for three iterations. Table \ref{tab:NrOfSAMPLESBATCH} presents the results for the experiment after three iterations with batch size 50000, 100000 and 150000 samples respectively. Since the number of file read is not the experimental workload in each iteration, the number of file read and the used files must be the same for each iteration. a01.edf is chosen as the control workload for the experiment. Figure \ref{fig:Figures/EDFIMPORT} presents the CPU usage for each workload.
\begin{table}
\centering
\begin{tabular}{|l|l|r|r|r|}
\hline
\multicolumn{2}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Responsive metrics\\(a01.edf-5,64MB)\end{tabular}}} & \multicolumn{3}{l|}{\begin{tabular}[c]{@{}c@{}}Experimental workload\\ Number of samples in batch\end{tabular}} \\ \cline{3-5} 
\multicolumn{2}{|l|}{} & 50000 & 100000 & 150000 \\ \hline
\multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}}CPU usage\\ (percent)\end{tabular}} & edfFIS1.txt & edfFIS2.txt & edfFIS3.txt \\ \hline
\multicolumn{2}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Power consumption\\ (percent)\end{tabular}}} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} \\
\multicolumn{2}{|l|}{} & 2.4 & 2.0 & 1.7 \\ \hline
\multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}}SQLite time usage\\ (millisecond)\end{tabular}} & 228 235 & 209 936 & 209 442 \\ \hline
\end{tabular}
\caption{Number of samples in batch}
\label{tab:NrOfSAMPLESBATCH}
\end{table}
\subsection{Results and discussion}
\begin{figure}
        \centering 
        \subfigure[Number of file]{\label{fig:EDFimportNRfiles}\includegraphics[width=70mm]{Figures/CPUusageImportEDFNRFILEs.png}}
        \subfigure[Buffer size]{\label{fig:EDFimportBuffer}\includegraphics[width=70mm]{Figures/EDFimportBUFFlength.png}}
        \caption{CPU usage for the EDF import experiment}
        \label{fig:Figures/EDFIMPORT}
\end{figure}
When increasing the number of files that are simultaneously read, the responsive metrics are linearly increasing. Although each input file is managed by a separate thread, it must wait for accessing and writing for the database file. The larger the number of file read, the longer the time to write to the database file, hence, the power consumption is increased. As presented in Figure \ref{fig:EDFimportNRfiles}, the CPU usage is increasing when the number of read file is increased, but it is quietly stable between 25\% to 30\%. It is because the waiting time for I/O much longer than the CPU time. In contrast, when increasing the batch size, the power consumption is decreased, because the database application reduces the overhead for writing data to the database file. However, the CPU usage is likely the same for each iteration, because the CPU time that is used to parse samples in the a01.edf file is mostly unchanged.\\
As presented in Table \ref{tab:NrOfSIMULTANEOUSELYREAD}, the EDF files are 18 times bigger when they are stored in the database file, because the EDF/EDF file formats do not include metadata in a sample while the database model fines metadata such as timestamp and the record id a sample belongs to. Moreover, each sample in a EDF file i limited to 2 bytes integer while a sample in the database is 8 bytes float. However, the metadata used in the database model provide many advantages for working with data, such as leveraging the advantages of SQL, easy to mixed multiple channels from different sources, etc.\\
The a01.edf file contains 2958000 samples, and the database application took 228235 milliseconds to insert the samples to the database. It is about 12960 samples per second. The time to read a EDF format file is therefore depended on the number samples the file contains. The time for reading process is decreasing when the number of sample in a EDF format file is increasing.\\
The resource usages are quietly efficient, but the performance is poor when importing EDF/EDF plus format files. It is because the files take turn to write their data to the database file. Parallel reading files does not increase the database writing performance, but it increase the performance for parsing data from EDF files.
\section{EDF exporting experiment}
The database application is required to export data to EDF file format for sharing or for backup data purposes. Since each record in the database is defined for a specific channel of a source (a.k.a channel record), and if records have the same patient id, physician id and timestamp, they are considered to be belonged to the same collecting process (a whole record). It is because a patient can use multiple sensor platforms to collect data. By choosing records with the same patient id, physician id and timestamp, a whole record can be retrieved. It is freely to choose which channel records to be included in the exported record; it is because sometimes a user needs data from some specific channels. EDF file format requires that a whole record need to be divided into small data records. A duration of the small data records is freely chosen. However, a data record size does not exceed 61440 bytes \citep{EDFpluss}. In the implementation of the database application, a data record is defined as a buffer in memory, and this buffer is flushed to the exported file when it is full. Hence, the data record size is also the buffer size which is defined by a summary of all channel buffer sizes. A channel buffer size is defined by multiplying the data record duration with the frequency of a channel.\\\\
$bufferSize=\sum_{i = 1}^{n}\frac{duration}{frequency\ channel(i)}$\\\\
Goals of the experiment are to evaluate the read performance of the database and resource usage such as percent CPU usage and battery consumption.
\subsection{Experiment workloads}
As presented in the design of the database application, the application continuously queries part of a record into a buffer, then writes the buffer to the exported EDF file. The application requests a database access for each selected record. If the number of selected records for exporting is large, the application needs to access the database multiple times. Therefore, experimental workloads for the experiment is the number of exporting records.
\subsection{Experiment metrics}
A record can be very large, since it can be collected in long time, and it takes time to export data. The database application needs to query data multiple times to avoid memory corruption since the record cannot be kept in memory for exporting. Therefore, metrics such as CPU usage, power consumption, database read performance, and shared SQLite connect time usage are chosen as the used metrics for the experiment.
\subsection{Experiment setup}
Since the experimental workloads for the experiment are the number of exporting records and the duration of a data record, by changing values of the experimental workloads, the EDF exporting experiment can be evaluated by assessing the results of the experiment. The records in the database are collected from the BITalino sensor kit in 60 minutes, and all channels of the BITalino are used. Each channel deliveries samples with frequency 100Hz. 
\begin{table}
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\multicolumn{2}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Responsive metrics\end{tabular}}} & \multicolumn{3}{l|}{\begin{tabular}[c]{@{}c@{}}Experimental workload\\ Number of exporting records\end{tabular}} \\ \cline{3-5} 
\multicolumn{2}{|l|}{} & 1 & 3 & 6 \\ \hline
\multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}}CPU usage\\ (percent)\end{tabular}} & edfFER1.txt & edfFER2.txt & edfFER3.txt \\ \hline
\multicolumn{2}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Power consumption\\ (percent)\end{tabular}}} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} \\
\multicolumn{2}{|l|}{} &  &  &  \\ \hline
\multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}}SQLite usage time\\ (millisecond)\end{tabular}} &  &  &  \\ \hline
\multicolumn{2}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Exported file size\\ (Megabytes)\end{tabular}}} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} \\
\multicolumn{2}{|l|}{} &  &  &  \\ \hline
\end{tabular}
\caption{Number of exporting records}
\label{tab:NrOfEXPORTINGRECORD}
\end{table}
Table \ref{tab:NrOfEXPORTINGRECORD} presents the results for the experiment where the experimental workload is the number of exporting record. The experiment is performed with 5, 10, and 15 records for each iteration. The data record duration that is used in each iteration is five seconds. In this table, data size in bytes for each experimental workload is calculated by multiplying total rows, that the exporting records occupier in the database, by the summary of data types from the returned columns. The pseudo SQLite code for calculating data size for returned rows is as following:\\\\
\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Pseudo SQLite code for calculating data size for returned rows}, label = {listing:SQLiteSizeCode}, captionpos=b, language=SQL, basicstyle=\small]
SELECT column A, column B, (count(*)*(size data type column A + size data type column B)) as datasize
FROM tables
WHERE conditions
\end{lstlisting}
\end{minipage}
\subsection{Results and discussion}
<WHEN I HAVE THE RESULTS, I WILL WRITE THIS SUBSECTION: HOW TO PLOT DATA... THEN, AS PRESENTED IN FIGURE ...>
\section{Querying data experiment}
When processing a SQL query, a database management system (DBMS) goes through at least three steps. The first step is to parse SQL query into a data structure that can be processed by the DBMS. In this step, the syntax and semantic of the query are checked to determine if the SQL statement is syntactic validity, and if the statement is meaningful, i.e. whether the columns and tables are existed in the database. The second step is to optimize the query. That is, by using a set of equivalence rules, the query optimizer can generate many equivalent plans in which a cost is assigned for each plan. The plan that has a lowest cost is taken to the third step which is to execute the query plan. Although the SQL query is optimized before querying, the complexity of the SQL query and the results from the query are not changed.\\
Goals of the experiment is to evaluate the resource usage and performance of the database application with respect to the complexity of the queries, and the size of the results.
\subsection{Experiment workloads}
In SQLite, all queries are simple select statement. A simple select statement is often processed via four steps \citep{SQLITE_SELECT}.
\begin{itemize}
\item From clause processing: The input data of the statement are specified in this step.
\item Where clause processing: The expression in the clause helps to filter the return data.
\item Group by, having processing: groups of data are calculating and aggregating with respect to the filter expression in having.
\item Distinct / ALL keyword processing: result rows are return (duplicate rows are removed if “distinct” is specified).
\end{itemize}
A compound query statement is defined via connecting multiple simple select statement by using UNION, UNION ALL, etc.\\
Workloads for the experiment are therefore depended on the FROM clause, and whether statements are compound statements. As presented in SQLite \citep{SQLITE_SELECT}, all join-operators are based on Cartesian product of the left hand side (N rows) and right hand side (M rows) of the datasets. The results from the join are therefore NxM rows. In other words, the complexity when joining two table is O(MxN). The complexity is increased by the number of tables that are stated in the FROM clause. Moreover, if the result from the query is too large to hold in the memory, the DBMS musts pass the database multiple times (multiple-passes).\\
\begin{table}
\centering
\begin{tabular}{|l|c|c|}
\hline
Table & Number of columns & Approximate records \\ \hline
SensorSource & 3 & \textless100 \\ \hline
Patient & 7 & \textless100 \\ \hline
Physician & 4 & \textless100 \\ \hline
Clinic & 5 & \textless100 \\ \hline
Person & 8 & \textless100 \\ \hline
Channel & 11 & \textless1000 \\ \hline
Record & 10 & \textless1000 \\ \hline
RecordAnnotation & 2 & \textless1000 \\ \hline
Annotation & 5 & \textless1000 \\ \hline
Sample & 3 & \textgreater 1 million \\ \hline
\end{tabular}
\caption{Datasets in the database}
\label{tab:dataSETs}
\end{table}
As presented in Table \ref{tab:dataSETs}, the heaviest table is the Sample table, and together with the complexity of queries, they are therefore considered the main workloads of the experiment. There are four queries that are used for evaluating the experiment; the queries are described as follow:
\begin{itemize}
\item \textbf{Query 1:} A relatively small dataset is scanned (a.k.a simple query with small result). Any tables presented in Table \ref{tab:dataSETs}, as long as not the table Sample, can be chosen for this query.
\item \textbf{Query 2:} A relatively large dataset is scanned (a.k.a simple query with large result). 
\item \textbf{Query 3:} All presented tables in Table \ref{tab:dataSETs} are joined, then an aggregate function, i.e. count, sum, max, etc., is applied to have a small result (a.k.a complex query with small result).
\item \textbf{Query 4:} All presented tables in Table \ref{tab:dataSETs} are joined, no filters are applied in this query (a.k.a complex query with large result).
\end{itemize}
\subsection{Experiment metrics}
Metrics that are used for evaluating the experiment must highlight the resource usage and performance of the database application when executing different queries with different complexity. Therefore, the metrics that are chosen for the experiment are percent CPU usage, power consumption, and response time for queries. 
\subsection{Experiment setup}
\begin{table}
\centering
\begin{tabular}{|c|l|c|c|c|c|}
\hline
\multicolumn{1}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Query\\ Number\end{tabular}}} & \multicolumn{1}{c|}{\multirow{2}{*}{Table(s)}} & \multicolumn{2}{c|}{Input size} & \multicolumn{2}{c|}{Output size} \\ \cline{3-6} 
\multicolumn{1}{|l|}{} & \multicolumn{1}{c|}{} & MB & \# records & MB & \# records \\ \hline
1 & Channel &  &  &  &  \\ \hline
2 & Sample &  &  &  &  \\ \hline
3 & All tables &  &  &  &  \\ \hline
4 & All tables &  &  &  &  \\ \hline
\end{tabular}
\caption{Query statistics for the queries}
\label{tab:QUERYSTATISTICS}
\end{table}
The described queries in the subsection workload of this experiment are translated into SQLite code as follow:
\begin{itemize}
\item \textbf{Query 1:} The whole table Channel is chosen to be scanned.\\\texttt{select * from Channel}
\item \textbf{Query 2:} Since the table Sample is the largest, it is chosen for this query.\\
\texttt{select * from Sample}
\item \textbf{Query 3:} This query counts number of rows return from Cartesian product all tables.\\
\texttt{select count(*) from SensorSource, Patient, Physician, Clinic, Person, Channel, Record, RecordAnnotation, Annotation, Sample}
\item \textbf{Query 4:} This query return all rows from Cartesian product all tables.\\
\texttt{select * from SensorSource, Patient, Physician, Clinic, Person, Channel, Record, RecordAnnotation, Annotation, Sample}
\end{itemize}
Table \ref{tab:QUERYSTATISTICS} presents query statistics after executing the queries, while Table \ref{tab:QUERYING} presents the results for the experiment.
\begin{table}
\centering
\begin{tabular}{|l|l|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Responsive metrics\end{tabular}}} & \multicolumn{4}{c|}{\begin{tabular}[c]{@{}c@{}}Experimental workload\end{tabular}} \\ \cline{3-6} 
\multicolumn{2}{|c|}{} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Simple query-\\ small result\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Simple query-\\ large result\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Complex query-\\ small result\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Complex query-\\ small result\end{tabular}} \\ \hline
\multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}}CPU usage\\ (percent)\end{tabular}} & text1.txt & text2.txt & text3.txt & text4.txt \\ \hline
\multicolumn{2}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Power consumption\\ (percent)\end{tabular}}} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} \\
\multicolumn{2}{|l|}{} &  &  &  &  \\ \hline
\multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}}Executing ime\\ (millisecond)\end{tabular}} &  &  &  &  \\ \hline
\end{tabular}
\caption{Executing time and resource usage for the queries}
\label{tab:QUERYING}
\end{table}
The records in the database are collected from the BITalino sensor kit in 60 minutes, and all channels of the BITalino are used. Each channel deliveries samples with frequency 100Hz. 
\subsection{Results and discussion}
<WHEN I HAVE THE RESULTS, I WILL WRITE THIS SUBSECTION: HOW TO PLOT DATA... THEN, AS PRESENTED IN FIGURE ...>
\section{Stress test with visualization and mixed tasks experiment}
It is possible that a user collects data from multiple sensor sources to the database, and plots the collected data on a graphical view while there is an exporting process is running. In other words, the application usually performs multiple tasks simultaneously rather than finishes a task before beginning the others.\\
Therefore, goals of the experiment are to evaluate database read/write performances, database size when collecting data from many different sources (both EDF files and CESAR acquisition tool), and resource usage such as CPU and battery consumption.
\subsection{Experiment workloads and metrics}
Workloads for this experiment are borrowed from the experiments above, which are buffer duration, arrival rate, number of collecting channel (these workloads come from the real-time experiment), number of file simultaneously read, number of sample in a batch (these workloads are from EDF importing experiment), number of exporting records, duration of a data record in the exported EDF file (from the EDF exporting experiment), and the complexity of queries a user want to execute.\\
Metrics that are chosen in this experiment must highlighting the database application performance, the size of the database, and resource usage. Therefore, percent CPU usage, power consumption, database size in megabytes, and SQLite time usage in millisecond for each task are chosen as metrics for the evaluation.
\subsection{Experiment setup}
Each experiment, which is presented the previous sections, is evaluated with three different workloads. These workloads are increased after each run. Workloads for this experiment are combined of the workloads for each iteration of the experiments. The workloads are used for three iterations (three run) respectively in both real-time and non-real-time visualizations. However, the experiment is not performed in six or twelve hours as in the real-time experiment, it is finished as long as the files are read and/or exported. The querying experiment is not included in this experiment. It is because an executing of a query is very fast compared with reading or writing process, and to repeat the same query to keep synchronization with reading or writing process in order to evaluate the experiment is not a good idea since the DBMS caches the result of the query.
\subsection{Results and discussion}
<WHEN I HAVE THE RESULTS, I WILL WRITE THIS SUBSECTION: HOW TO PLOT DATA... THEN, AS PRESENTED IN FIGURE ...>
\begin{table}
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Workloads\\ (real-time plotting)\end{tabular}}} & \multicolumn{3}{c|}{Responsive metrics} \\ \cline{2-4} 
\multicolumn{1}{|c|}{} & \begin{tabular}[c]{@{}c@{}}CPU usage \\ (percent)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Power consumption \\ (percent)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Database size \\ (Megabytes)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Buffer duration: 15s\\ Arrival rate: 100Hz\\ Nr. channel: 12\\ File read: 3\\ Batch size: 150k\\ Nr. exporting record: 15\\ Data record duration: 10s\end{tabular} &  &  &  \\ \hline
\end{tabular}
\caption{Mixed tasks with real-time visualization}
\label{concept2}
\end{table}
\begin{table}
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Workloads\\ (with plotting data \\ from the database)\end{tabular}}} & \multicolumn{3}{c|}{Responsive metrics} \\ \cline{2-4} 
\multicolumn{1}{|c|}{} & \begin{tabular}[c]{@{}c@{}}CPU usage \\ (percent)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Power consumption \\ (percent)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Database size \\ (Megabytes)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}Buffer duration: 15s\\ Arrival rate: 100Hz\\ Nr. channel: 12\\ File read: 3\\ Batch size: 150k\\ Nr. exporting record: 15\\ Data record duration: 10s\end{tabular} &  &  &  \\ \hline
\end{tabular}
\caption{Mixed tasks with visualization data from the database}
\label{concept3}
\end{table}
